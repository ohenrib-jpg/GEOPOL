"""
D√©tecteur d'anomalies SDR √† faible latence.

Principe :
- Analyse spectrale uniquement (pas d'√©coute audio)
- D√©tection bas√©e sur √©carts statistiques (moyenne mobile + √©cart-type)
- Latence cible : < 5 minutes
- Objectif : D√©tecter √©v√©nements anthropiques soudains

Version: 1.0.0
Author: GEOPOL Analytics
"""

from enum import Enum
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
import statistics
import logging

logger = logging.getLogger(__name__)


class AnomalyLevel(Enum):
    """Niveaux de s√©v√©rit√© des anomalies."""
    INFO = "INFO"              # < 2œÉ - Variations normales
    WARNING = "WARNING"        # 2-3œÉ - √Ä surveiller
    HIGH_RISK = "HIGH_RISK"    # 3-4œÉ - Anomalie probable
    CRITICAL = "CRITICAL"      # > 4œÉ - Anomalie confirm√©e


@dataclass
class AnomalyDetectionConfig:
    """Configuration de la d√©tection d'anomalies."""

    # Fen√™tre temporelle pour la baseline (en heures)
    baseline_window_hours: int = 24

    # Seuils en nombre d'√©carts-types (sigma)
    threshold_warning: float = 2.0
    threshold_high_risk: float = 3.0
    threshold_critical: float = 4.0

    # Fen√™tre minimale de donn√©es pour d√©tecter (en points)
    min_data_points: int = 10

    # Latence de d√©tection (en minutes)
    detection_interval_minutes: int = 5

    # Seuils sp√©cifiques par zone (optionnel)
    zone_thresholds: Dict[str, Dict[str, float]] = field(default_factory=dict)


@dataclass
class Anomaly:
    """Repr√©sente une anomalie d√©tect√©e."""

    timestamp: datetime
    zone_id: str
    zone_name: str
    metric_name: str
    current_value: float
    baseline_value: float
    std_deviation: float
    sigma_deviation: float  # Nombre d'√©carts-types
    level: AnomalyLevel
    description: str
    metadata: Dict = field(default_factory=dict)

    def to_dict(self) -> Dict:
        """Convertit en dictionnaire."""
        return {
            'timestamp': self.timestamp.isoformat(),
            'zone_id': self.zone_id,
            'zone_name': self.zone_name,
            'metric_name': self.metric_name,
            'current_value': self.current_value,
            'baseline_value': self.baseline_value,
            'std_deviation': self.std_deviation,
            'sigma_deviation': round(self.sigma_deviation, 2),
            'level': self.level.value,
            'description': self.description,
            'metadata': self.metadata
        }


class AnomalyDetector:
    """
    D√©tecteur d'anomalies SDR bas√© sur analyse statistique.

    Algorithme :
    1. Calcul baseline (moyenne mobile sur 24h)
    2. Calcul √©cart-type pour mesurer volatilit√©
    3. D√©tection bas√©e sur nombre de sigma
    4. Classification par niveau de s√©v√©rit√©
    """

    def __init__(self, config: Optional[AnomalyDetectionConfig] = None):
        """
        Initialise le d√©tecteur d'anomalies.

        Args:
            config: Configuration optionnelle (utilise d√©faut si None)
        """
        self.config = config or AnomalyDetectionConfig()
        logger.info(f"üîç AnomalyDetector initialis√© (baseline={self.config.baseline_window_hours}h)")

    def detect_anomalies(
        self,
        zone_id: str,
        zone_name: str,
        current_metrics: Dict[str, float],
        historical_data: List[Dict[str, float]],
        timestamp: Optional[datetime] = None
    ) -> List[Anomaly]:
        """
        D√©tecte les anomalies pour une zone g√©ographique.

        Args:
            zone_id: Identifiant de la zone
            zone_name: Nom de la zone
            current_metrics: M√©triques actuelles {metric_name: value}
            historical_data: Donn√©es historiques (liste de dicts)
            timestamp: Timestamp actuel (d√©faut: maintenant)

        Returns:
            Liste des anomalies d√©tect√©es
        """
        if timestamp is None:
            timestamp = datetime.utcnow()

        anomalies = []

        # V√©rifier qu'on a assez de donn√©es
        if len(historical_data) < self.config.min_data_points:
            logger.warning(
                f"Pas assez de donn√©es pour {zone_name} "
                f"({len(historical_data)} < {self.config.min_data_points})"
            )
            return anomalies

        # Analyser chaque m√©trique
        for metric_name, current_value in current_metrics.items():
            anomaly = self._detect_metric_anomaly(
                zone_id=zone_id,
                zone_name=zone_name,
                metric_name=metric_name,
                current_value=current_value,
                historical_data=historical_data,
                timestamp=timestamp
            )

            if anomaly:
                anomalies.append(anomaly)

        return anomalies

    def _detect_metric_anomaly(
        self,
        zone_id: str,
        zone_name: str,
        metric_name: str,
        current_value: float,
        historical_data: List[Dict[str, float]],
        timestamp: datetime
    ) -> Optional[Anomaly]:
        """
        D√©tecte une anomalie pour une m√©trique sp√©cifique.

        Args:
            zone_id: ID de la zone
            zone_name: Nom de la zone
            metric_name: Nom de la m√©trique
            current_value: Valeur actuelle
            historical_data: Donn√©es historiques
            timestamp: Timestamp

        Returns:
            Anomaly si d√©tect√©e, None sinon
        """
        # Extraire les valeurs historiques pour cette m√©trique
        historical_values = [
            data.get(metric_name, 0)
            for data in historical_data
            if metric_name in data
        ]

        if len(historical_values) < self.config.min_data_points:
            return None

        # Calculer la baseline (moyenne)
        baseline = statistics.mean(historical_values)

        # Calculer l'√©cart-type
        try:
            std_dev = statistics.stdev(historical_values)
        except statistics.StatisticsError:
            # Pas assez de variation dans les donn√©es
            std_dev = 0.0

        # Si √©cart-type = 0, toutes les valeurs sont identiques
        if std_dev == 0:
            # D√©tecter uniquement si la valeur actuelle est diff√©rente
            if current_value != baseline:
                sigma = float('inf')
            else:
                return None
        else:
            # Calculer le nombre d'√©carts-types
            sigma = abs(current_value - baseline) / std_dev

        # R√©cup√©rer les seuils (sp√©cifiques √† la zone ou par d√©faut)
        zone_thresholds = self.config.zone_thresholds.get(zone_id, {})
        threshold_warning = zone_thresholds.get('warning', self.config.threshold_warning)
        threshold_high_risk = zone_thresholds.get('high_risk', self.config.threshold_high_risk)
        threshold_critical = zone_thresholds.get('critical', self.config.threshold_critical)

        # D√©terminer le niveau d'anomalie
        level = None
        if sigma >= threshold_critical:
            level = AnomalyLevel.CRITICAL
        elif sigma >= threshold_high_risk:
            level = AnomalyLevel.HIGH_RISK
        elif sigma >= threshold_warning:
            level = AnomalyLevel.WARNING

        # Si pas d'anomalie, retourner None
        if level is None:
            return None

        # Cr√©er la description
        direction = "augmentation" if current_value > baseline else "diminution"
        percentage = abs((current_value - baseline) / baseline * 100) if baseline != 0 else 0

        description = (
            f"{level.value}: {direction} anormale de {metric_name} "
            f"dans {zone_name} ({percentage:.1f}% vs baseline, {sigma:.1f}œÉ)"
        )

        # Cr√©er l'anomalie
        anomaly = Anomaly(
            timestamp=timestamp,
            zone_id=zone_id,
            zone_name=zone_name,
            metric_name=metric_name,
            current_value=current_value,
            baseline_value=baseline,
            std_deviation=std_dev,
            sigma_deviation=sigma,
            level=level,
            description=description,
            metadata={
                'percentage_change': round(percentage, 1),
                'direction': direction,
                'data_points': len(historical_values)
            }
        )

        logger.info(f"üö® {description}")

        return anomaly

    def calculate_baseline(
        self,
        metric_name: str,
        historical_data: List[Dict[str, float]]
    ) -> Tuple[float, float]:
        """
        Calcule la baseline et l'√©cart-type pour une m√©trique.

        Args:
            metric_name: Nom de la m√©trique
            historical_data: Donn√©es historiques

        Returns:
            (baseline, std_deviation)
        """
        values = [
            data.get(metric_name, 0)
            for data in historical_data
            if metric_name in data
        ]

        if len(values) < 2:
            return 0.0, 0.0

        baseline = statistics.mean(values)

        try:
            std_dev = statistics.stdev(values)
        except statistics.StatisticsError:
            std_dev = 0.0

        return baseline, std_dev

    def filter_recent_data(
        self,
        historical_data: List[Dict],
        hours: Optional[int] = None
    ) -> List[Dict]:
        """
        Filtre les donn√©es pour ne garder que la fen√™tre temporelle r√©cente.

        Args:
            historical_data: Donn√©es historiques avec timestamp
            hours: Nombre d'heures (d√©faut: config.baseline_window_hours)

        Returns:
            Donn√©es filtr√©es
        """
        if hours is None:
            hours = self.config.baseline_window_hours

        cutoff_time = datetime.utcnow() - timedelta(hours=hours)

        filtered = [
            data for data in historical_data
            if 'timestamp' in data and
               datetime.fromisoformat(data['timestamp']) >= cutoff_time
        ]

        return filtered

    def get_anomaly_summary(
        self,
        anomalies: List[Anomaly]
    ) -> Dict:
        """
        G√©n√®re un r√©sum√© des anomalies d√©tect√©es.

        Args:
            anomalies: Liste des anomalies

        Returns:
            Dictionnaire de statistiques
        """
        if not anomalies:
            return {
                'total': 0,
                'by_level': {},
                'by_zone': {},
                'critical_count': 0,
                'high_risk_count': 0,
                'warning_count': 0
            }

        # Compter par niveau
        by_level = {}
        for level in AnomalyLevel:
            by_level[level.value] = sum(1 for a in anomalies if a.level == level)

        # Compter par zone
        by_zone = {}
        for anomaly in anomalies:
            zone_name = anomaly.zone_name
            if zone_name not in by_zone:
                by_zone[zone_name] = 0
            by_zone[zone_name] += 1

        return {
            'total': len(anomalies),
            'by_level': by_level,
            'by_zone': by_zone,
            'critical_count': by_level.get('CRITICAL', 0),
            'high_risk_count': by_level.get('HIGH_RISK', 0),
            'warning_count': by_level.get('WARNING', 0),
            'info_count': by_level.get('INFO', 0)
        }
