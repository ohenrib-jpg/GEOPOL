# Flask/llama_client.py - VERSION COMPL√àTEMENT CORRIG√âE POUR MISTRAL 7B
# Flask/llama_client.py - VERSION COMPL√àTEMENT CORRIG√âE POUR MISTRAL 7B - AVEC CHAT
"""
Client Python optimis√© pour Mistral 7B v0.2 Q4_0
Configuration CPU Ryzen 5 5600U, 16GB RAM
"""

import logging
import requests
import json
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import time
import hashlib

logger = logging.getLogger(__name__)


class LlamaClient:
    """Client optimis√© pour Mistral 7B v0.2 Q4_0 avec configuration CPU"""
    
    def __init__(self, endpoint: str = "http://localhost:8080", timeout: int = 600):
        self.endpoint = endpoint.rstrip('/')
        self.timeout = timeout
        self.max_retries = 2
        self.retry_delay = 3

        # Configuration optimis√©e pour CPU Ryzen 5 5600U (SANS GPU)
        self.model_configs = {
            'default': {
                'temperature': 0.3,
                'top_p': 0.8,
                'top_k': 40,
                'max_tokens': 1200,  # R√©duit de 1500 pour acc√©l√©rer
                'repeat_penalty': 1.1,
                'stop': ["</s>", "[INST]", "[/INST]"],
                'threads': 6  # R√©duit de 10 pour √©viter contention CPU
            },
            'chat': {
                'temperature': 0.4,
                'top_p': 0.8,
                'max_tokens': 350,  # R√©duit de 400 pour acc√©l√©rer
                'threads': 4  # R√©duit de 8 pour √©viter contention CPU
            }
        }
    
    def test_connection(self) -> Tuple[bool, str]:
        """Teste la connexion au serveur Llama"""
        try:
            start_time = time.time()
            response = requests.get(
                f"{self.endpoint}/health",
                timeout=15
            )
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                logger.info(f"‚úÖ Connexion Mistral 7B √©tablie ({response_time:.2f}s)")
                return True, f"Connect√© ({response_time:.2f}s)"
            else:
                logger.warning(f"‚ö†Ô∏è Serveur Mistral r√©pond avec statut {response.status_code}")
                return False, f"Statut {response.status_code}"
                
        except requests.exceptions.ConnectionError:
            logger.error("‚ùå Impossible de se connecter au serveur Mistral")
            return False, "Erreur de connexion"
        except requests.exceptions.Timeout:
            logger.error("‚è±Ô∏è Timeout de connexion au serveur Mistral")
            return False, "Timeout de connexion"
        except Exception as e:
            logger.error(f"‚ùå Erreur inattendue lors du test de connexion: {e}")
            return False, f"Erreur: {str(e)}"
    
    def _build_mistral_prompt(self, system_prompt: str, user_prompt: str) -> str:
        """Construit le prompt au format Mistral [INST]...[/INST] SANS <s>"""
        # SUPPRIMER le <s> initial pour √©viter le double BOS token
        return f"[INST] {system_prompt}\n\n{user_prompt} [/INST]"
    
    def _build_geopolitique_prompt(self, articles: List[Dict], context: Dict) -> Tuple[str, str]:
        """Construit le prompt pour analyse g√©opolitique - FORMAT MISTRAL"""
        
        sentiment_summary = self._build_sentiment_summary(context)
        articles_summary = self._build_articles_summary(articles)
        themes_text = self._build_themes_text(context)
        
        system_prompt = """Tu es un analyste g√©opolitique d√©pressif et d√©faitiste sp√©cialis√© dans la comparaison des sources m√©diatiques et des sources populaires. 
Ta mission est de produire des rapports structur√©s, factuels et ironiques bas√©s exclusivement sur les articles fournis.

DIRECTIVES CRITIQUES:
- Base ton analyse UNIQUEMENT sur les articles fournis
- Sois factuel, triste, et professionnel
- Structure ta r√©ponse avec des titres Markdown
- Cite des exemples concrets des articles quand c'est pertinent
- Tu es oblig√© d‚Äôinclure tes sp√©culations et tes opinions personnelles
- Utilise un fran√ßais acad√©mique et pr√©cis"""

        user_prompt = f"""Analyse les articles suivants et produits un rapport g√©opolitique structur√©.

CONTEXTE:
‚Ä¢ P√©riode: {context.get('period', 'Non sp√©cifi√©e')}
‚Ä¢ Articles analys√©s: {len(articles)}
‚Ä¢ Th√®mes: {themes_text}

DISTRIBUTION DES SENTIMENTS:
{sentiment_summary}

ARTICLES √Ä ANALYSER:
{articles_summary}

STRUCTURE DEMAND√âE:

## 1. SYNTH√àSE EX√âCUTIVE
[R√©sum√© des conclusions majeures en 2-3 paragraphes]

## 2. TENDANCES G√âOPOLITIQUES PRINCIPALES
- Identifier 3-4 tendances structurelles √©mergentes
- Pour chaque tendance : contexte, acteurs cl√©s, implications

## 3. POINTS DE TENSION ET CONFLITS
- Cartographier les zones de tension actuelles
- Analyser l'√©volution r√©cente des conflits

## 4. ACTEURS ET RAPPORTS DE FORCE
- Analyser les strat√©gies des grandes puissances
- √âvaluer les dynamiques d'alliances

## 5. RECOMMANDATIONS STRAT√âGIQUES
- Actions de veille prioritaires
- Sc√©narios probables √† 3-6 mois

Longueur: 800-1200 mots maximum.
Commence directement par "## 1. SYNTH√àSE EX√âCUTIVE"."""
        
        return system_prompt, user_prompt
    
    def _build_economique_prompt(self, articles: List[Dict], context: Dict) -> Tuple[str, str]:
        """Construit le prompt pour analyse √©conomique - FORMAT MISTRAL"""
        
        articles_summary = self._build_articles_summary(articles)
        
        system_prompt = """Tu es un analyste √©conomique sp√©cialis√© dans l'analyse des march√©s et politiques √©conomiques.
Produis des analyses factuelles bas√©es sur les donn√©es fournies, sans sp√©culation."""

        user_prompt = f"""Analyse √©conomique des articles suivants:

CONTEXTE:
‚Ä¢ P√©riode: {context.get('period', 'Non sp√©cifi√©e')}
‚Ä¢ Articles analys√©s: {len(articles)}

ARTICLES:
{articles_summary}

PRODUIS UN RAPPORT √âCONOMIQUE STRUCTUR√â:

## 1. INDICATEURS MACRO√âCONOMIQUES
- Tendances de croissance, inflation, commerce
- Dynamiques des march√©s financiers

## 2. POLITIQUES √âCONOMIQUES
- D√©cisions politiques majeures
- Impacts sur l'√©conomie r√©elle

## 3. RISQUES SYST√âMIQUES
- Dettes souveraines, d√©s√©quilibres
- D√©pendances strat√©giques

## 4. RECOMMANDATIONS OP√âRATIONNELLES
- Strat√©gies d'adaptation
- Opportunit√©s d'investissement

Base ton analyse sur les donn√©es fournies.
Longueur: 600-900 mots."""
        
        return system_prompt, user_prompt
    
    def _build_securite_prompt(self, articles: List[Dict], context: Dict) -> Tuple[str, str]:
        """Construit le prompt pour analyse s√©curit√© - FORMAT MISTRAL"""
        
        articles_summary = self._build_articles_summary(articles[:8])  # R√©duit pour CPU
        
        system_prompt = """Tu es un analyste en s√©curit√© g√©opolitique. 
Produis des briefings factuels et op√©rationnels bas√©s sur les informations disponibles."""

        user_prompt = f"""Briefing s√©curitaire bas√© sur les articles:

CONTEXTE:
P√©riode: {context.get('period', 'Non sp√©cifi√©e')}
Articles: {len(articles)}

INFORMATIONS:
{articles_summary}

STRUCTURE DU BRIEFING:

## 1. √âVALUATION DES MENACES IMM√âDIATES
- Menaces terroristes, cyberattaques, conflits
- Niveau d'alerte par r√©gion

## 2. DYNAMIQUES DES ACTEURS NON-√âTATIQUES
- Groupes arm√©s, organisations criminelles
- Capacit√©s et intentions

## 3. CAPACIT√âS D√âFENSIVES
- Mesures de s√©curit√© d√©ploy√©es
- Gaps capacitaires identifi√©s

## 4. RECOMMANDATIONS OP√âRATIONNELLES
- Mesures de protection imm√©diates
- Zones √† s√©curiser en priorit√©

Ton professionnel et factuel. 400-700 mots."""
        
        return system_prompt, user_prompt
    
    def _build_synthese_prompt(self, articles: List[Dict], context: Dict) -> Tuple[str, str]:
        """Construit le prompt pour synth√®se hebdomadaire - FORMAT MISTRAL"""
        
        articles_summary = self._build_articles_summary(articles[:10])  # R√©duit pour CPU
        
        system_prompt = """Tu es un analyste de veille m√©diatique. 
Produis des synth√®ses concises et informatives des actualit√©s de la semaine."""

        user_prompt = f"""Synth√®se hebdomadaire des actualit√©s:

P√âRIODE: {context.get('period', 'Derni√®re semaine')}
{len(articles)} articles analys√©s

FAITS SAILLANTS:
{articles_summary}

PRODUIS UNE SYNTH√àSE STRUCTUR√âE:

## 1. √âV√âNEMENTS MAJEURS DE LA SEMAINE
[3-5 √©v√©nements maximum]

## 2. TENDANCES SIGNIFICATIVES
- √âvolutions politiques, √©conomiques, sociales

## 3. ANALYSE G√âOPOLITIQUE
- √âquilibres de pouvoir et relations internationales

## 4. PERSPECTIVES ET AGENDA
- √âv√©nements √† surveiller la semaine prochaine

Style concis et informatif. 300-500 mots."""
        
        return system_prompt, user_prompt
    
    def _build_sentiment_summary(self, context: Dict) -> str:
        """Construit le r√©sum√© des sentiments"""
        positive = context.get('sentiment_positive', 0)
        negative = context.get('sentiment_negative', 0)
        neutral = context.get('sentiment_neutral', 0)
        neutral_positive = context.get('sentiment_neutral_positive', 0)
        neutral_negative = context.get('sentiment_neutral_negative', 0)
        total = context.get('total_articles', 1)
        
        return f"""
‚Ä¢ Positifs: {positive} ({positive/total*100:.1f}%)
‚Ä¢ L√©g√®rement positifs: {neutral_positive} ({neutral_positive/total*100:.1f}%)
‚Ä¢ Neutres: {neutral} ({neutral/total*100:.1f}%)
‚Ä¢ L√©g√®rement n√©gatifs: {neutral_negative} ({neutral_negative/total*100:.1f}%)
‚Ä¢ N√©gatifs: {negative} ({negative/total*100:.1f}%)"""
    
    def _build_articles_summary(self, articles: List[Dict], max_articles: int = 8) -> str:
        """Construit le r√©sum√© des articles (optimis√© CPU)"""
        if not articles:
            return "Aucun article significatif √† analyser."
        
        summary = []
        for i, article in enumerate(articles[:max_articles]):
            source = article.get('source', 'Source inconnue')
            sentiment = article.get('detailed_sentiment') or article.get('sentiment', 'neutral')
            # Version courte pour √©conomiser des tokens
            title = article['title'][:100] + "..." if len(article['title']) > 100 else article['title']
            summary.append(f"{i+1}. {title} [{source}]")
        
        return "\n".join(summary)
    
    def _build_themes_text(self, context: Dict) -> str:
        """Construit le texte des th√®mes"""
        themes = context.get('themes', [])
        if not themes:
            return "Tous th√®mes confondus"
        return ", ".join(themes[:3]) if isinstance(themes, list) else str(themes)  # Limit√© √† 3 th√®mes
    
    def _debug_response(self, raw_response: str, cleaned_response: str):
        """Affiche des informations de d√©bogage sur la r√©ponse"""
        logger.info("üêõ D√âBOGAGE R√âPONSE MISTRAL:")
        logger.info(f"üìè Brut: {len(raw_response)} caract√®res")
        logger.info(f"üìè Nettoy√©: {len(cleaned_response)} caract√®res")
        logger.info(f"üìÑ D√©but brut: {raw_response[:200]}...")
        logger.info(f"üìÑ D√©but nettoy√©: {cleaned_response[:200]}...")
        
        # Sauvegarder pour analyse
        try:
            with open("debug_mistral_response.txt", "w", encoding="utf-8") as f:
                f.write("=== R√âPONSE BRUTE ===\n")
                f.write(raw_response)
                f.write("\n\n=== R√âPONSE NETTOY√âE ===\n")
                f.write(cleaned_response)
            logger.info("üíæ R√©ponse sauvegard√©e dans debug_mistral_response.txt")
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde debug: {e}")
    
    def _clean_mistral_response(self, text: str) -> str:
        """Nettoie la r√©ponse Mistral - VERSION AM√âLIOR√âE"""
        if not text:
            return ""
        
        # Supprimer les balises Mistral
        text = text.replace('</s>', '').replace('<s>', '')
        text = text.replace('[INST]', '').replace('[/INST]', '')
        
        # Nettoyer ligne par ligne
        lines = text.split('\n')
        clean_lines = []
        
        for line in lines:
            clean_line = line.strip()
            
            # Ignorer les lignes vides ou techniques
            if not clean_line:
                continue
                
            # Ignorer les r√©p√©titions de prompt
            if any(marker in clean_line for marker in [
                "Tu es un analyste", "DIRECTIVES CRITIQUES", 
                "Base ton analyse", "STRUCTURE DEMAND√âE",
                "SYST√àME:", "USER:"
            ]):
                continue
                
            # Garder les lignes de contenu
            clean_lines.append(clean_line)
        
        result = '\n'.join(clean_lines).strip()
        
        # Si le r√©sultat semble tronqu√©, essayer une m√©thode alternative
        if len(result) < 100:
            # M√©thode de secours: prendre tout apr√®s le dernier [/INST]
            parts = text.split('[/INST]')
            if len(parts) > 1:
                result = parts[-1].strip()
                # Nettoyer √† nouveau
                result = result.replace('</s>', '').replace('<s>', '')
                result = result.replace('[INST]', '').replace('assistant:', '')
        
        logger.info(f"üîß Nettoyage: {len(text)} ‚Üí {len(result)} caract√®res")
        return result
    
    def _validate_response(self, text: str, min_length: int = 100) -> bool:
        """Valide que la r√©ponse est utilisable - VERSION ASSOUPLIE"""
        if not text or len(text) < min_length:
            logger.warning(f"‚ùå R√©ponse trop courte: {len(text)} caract√®res")
            return False
        
        # V√©rifier que ce n'est pas une r√©p√©tition du prompt syst√®me
        prompt_indicators = [
            "Tu es un analyste g√©opolitique professionnel",
            "DIRECTIVES CRITIQUES:",
            "Base ton analyse UNIQUEMENT", 
            "STRUCTURE DEMAND√âE:",
            "[INST]", "[/INST]"
        ]
        
        for indicator in prompt_indicators:
            if indicator in text[:500]:  # V√©rifier seulement le d√©but
                logger.warning(f"‚ùå R√©ponse contient du prompt syst√®me: {indicator}")
                return False
        
        # V√©rifier qu'il y a du contenu substantiel
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        if len(lines) < 3:
            logger.warning("‚ùå Pas assez de lignes de contenu")
            return False
            
        logger.info(f"‚úÖ R√©ponse valid√©e: {len(text)} caract√®res, {len(lines)} lignes")
        return True
    
    def _make_llama_request(self, system_prompt: str, user_prompt: str, config: Dict) -> Dict:
        """Effectue la requ√™te vers le serveur Llama - VERSION AVEC D√âBOGAGE"""
        
        # Construction du prompt au format Mistral
        full_prompt = self._build_mistral_prompt(system_prompt, user_prompt)
        
        request_data = {
            "prompt": full_prompt,
            "temperature": config.get('temperature', 0.1),
            "top_p": config.get('top_p', 0.8),
            "top_k": config.get('top_k', 40),
            "max_tokens": config.get('max_tokens', 1500),
            "repeat_penalty": config.get('repeat_penalty', 1.1),
            "stop": config.get('stop', ["</s>", "[INST]", "[/INST]"]),
            "stream": False,
            "threads": config.get('threads', 10)
        }
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f"ü¶ô Tentative {attempt + 1}/{self.max_retries} vers {self.endpoint}")
                logger.info(f"üìä Configuration: {config.get('max_tokens')} tokens, temp {config.get('temperature')}")
                
                response = requests.post(
                    f"{self.endpoint}/completion",
                    json=request_data,
                    headers={"Content-Type": "application/json"},
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    data = response.json()
                    raw_response = data.get('content', '').strip()
                    
                    # NETTOYAGE
                    analysis_text = self._clean_mistral_response(raw_response)
                    
                    # D√âBOGAGE
                    self._debug_response(raw_response, analysis_text)
                    
                    if self._validate_response(analysis_text):
                        logger.info(f"‚úÖ R√©ponse Mistral valide ({len(analysis_text)} caract√®res)")
                        return {
                            'success': True,
                            'analysis': analysis_text,
                            'model_used': data.get('model', 'mistral-7b-v0.2-q4_0'),
                            'prompt_tokens': len(full_prompt.split()),
                            'completion_tokens': len(analysis_text.split()),
                            'config_used': config
                        }
                    else:
                        logger.warning("‚ö†Ô∏è R√©ponse Mistral invalide selon les crit√®res")
                        # SAUVEGARDER M√äME EN CAS D'√âCHEC POUR ANALYSE
                        try:
                            with open("failed_response.txt", "w", encoding="utf-8") as f:
                                f.write(f"Prompt: {user_prompt}\n\n")
                                f.write(f"R√©ponse brute: {raw_response}\n\n")
                                f.write(f"R√©ponse nettoy√©e: {analysis_text}")
                            logger.info("üíæ √âchec sauvegard√© dans failed_response.txt")
                        except Exception as e:
                            logger.error(f"‚ùå Erreur sauvegarde √©chec: {e}")
                            
                        if attempt < self.max_retries - 1:
                            time.sleep(self.retry_delay * (attempt + 1))
                            continue
                        else:
                            raise Exception("R√©ponse invalide apr√®s tous les essais")
                
                else:
                    logger.error(f"‚ùå Erreur HTTP {response.status_code}")
                    if attempt < self.max_retries - 1:
                        time.sleep(self.retry_delay * (attempt + 1))
                        continue
                    else:
                        raise Exception(f"Erreur HTTP {response.status_code}")
                        
            except requests.Timeout:
                logger.error(f"‚è±Ô∏è Timeout lors de la tentative {attempt + 1}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay * (attempt + 1))
                    continue
                else:
                    raise Exception("Timeout apr√®s tous les essais")
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur lors de la tentative {attempt + 1}: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay * (attempt + 1))
                    continue
                else:
                    raise
        
        raise Exception("√âchec apr√®s tous les essais de reconnexion")
    
    def _select_relevant_articles(self, articles: List[Dict], report_type: str, max_articles: int = 8) -> List[Dict]:
        """S√©lectionne les articles les plus pertinents (optimisation CPU sans GPU)"""
        if len(articles) <= max_articles:
            return articles
        
        # Prioriser les articles r√©cents et avec sentiment marqu√©
        scored_articles = []
        for article in articles:
            score = 0
            # Bonus pour les articles r√©cents
            if 'pub_date' in article:
                score += 10
            
            # Bonus pour les sentiments marqu√©s (positif ou n√©gatif)
            sentiment = article.get('detailed_sentiment') or article.get('sentiment', 'neutral')
            if sentiment in ['positive', 'negative']:
                score += 5
            elif sentiment in ['neutral_positive', 'neutral_negative']:
                score += 2
                
            # Bonus selon le type de rapport
            if report_type in article.get('title', '').lower():
                score += 3
                
            scored_articles.append((score, article))
        
        # Trier par score et prendre les meilleurs
        scored_articles.sort(key=lambda x: x[0], reverse=True)
        return [article for score, article in scored_articles[:max_articles]]
    
    def generate_analysis(self, report_type: str, articles: List[Dict],
                         context: Dict) -> Dict:
        """
        G√©n√®re une analyse avec Mistral 7B avec gestion robuste des erreurs
        """
        
        # V√©rifier la connexion
        connection_ok, connection_msg = self.test_connection()
        if not connection_ok:
            logger.warning(f"‚ö†Ô∏è Serveur Mistral inaccessible - {connection_msg}")
            return {
                'success': False,
                'error': f'Serveur Mistral inaccessible: {connection_msg}',
                'analysis': self._generate_fallback_analysis(report_type, articles, context),
                'connection_status': connection_msg,
                'model_used': 'fallback'
            }
        
        try:
            # S√©lectionner les articles les plus pertinents (optimisation CPU)
            relevant_articles = self._select_relevant_articles(articles, report_type)
            
            # Construire le prompt selon le type
            if report_type == 'geopolitique':
                system_prompt, user_prompt = self._build_geopolitique_prompt(relevant_articles, context)
            elif report_type == 'economique':
                system_prompt, user_prompt = self._build_economique_prompt(relevant_articles, context)
            elif report_type == 'securite':
                system_prompt, user_prompt = self._build_securite_prompt(relevant_articles, context)
            elif report_type == 'synthese':
                system_prompt, user_prompt = self._build_synthese_prompt(relevant_articles, context)
            else:
                system_prompt, user_prompt = self._build_geopolitique_prompt(relevant_articles, context)
            
            # Configuration du mod√®le
            model_config = self.model_configs.get(report_type, self.model_configs['default'])
            
            logger.info(f"ü¶ô G√©n√©ration d'analyse {report_type} avec {len(relevant_articles)} articles")
            
            # Appel au serveur
            result = self._make_llama_request(system_prompt, user_prompt, model_config)
            
            logger.info(f"‚úÖ Analyse {report_type} g√©n√©r√©e avec succ√®s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur critique lors de la g√©n√©ration: {e}")
            return {
                'success': False,
                'error': f'Erreur de g√©n√©ration: {str(e)}',
                'analysis': self._generate_fallback_analysis(report_type, articles, context),
                'connection_status': 'Erreur pendant la g√©n√©ration',
                'model_used': 'fallback'
            }
    
    def _generate_fallback_analysis(self, report_type: str, 
                                    articles: List[Dict],
                                    context: Dict) -> str:
        """
        G√©n√®re une analyse de secours d√©taill√©e (mode d√©grad√©)
        """
        
        sentiment_counts = {
            'positive': 0, 'negative': 0, 'neutral': 0, 
            'neutral_positive': 0, 'neutral_negative': 0
        }
        
        sources = {}
        themes = context.get('themes', [])
        
        for article in articles:
            sentiment = article.get('detailed_sentiment') or article.get('sentiment', 'neutral')
            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
            
            source = article.get('source', 'Source inconnue')
            sources[source] = sources.get(source, 0) + 1
        
        total_articles = len(articles)
        
        top_sources = sorted(sources.items(), key=lambda x: x[1], reverse=True)[:5]
        recent_articles = sorted(articles, key=lambda x: x.get('pub_date', ''), reverse=True)[:5]
        
        analysis = f"""
## RAPPORT {report_type.upper()} - MODE D√âGRAD√â

**‚ö†Ô∏è NOTE:** Ce rapport a √©t√© g√©n√©r√© en mode d√©grad√©. Le serveur d'analyse IA Mistral 7B est temporairement indisponible.

### üìä M√âTRIQUES GLOBALES

**P√©riode analys√©e:** {context.get('period', 'Non sp√©cifi√©e')}  
**Articles trait√©s:** {total_articles}  
**Th√®mes couverts:** {', '.join(themes) if themes else 'Tous th√®mes'}

### üìà ANALYSE DES SENTIMENTS

| Cat√©gorie | Nombre | Pourcentage |
|-----------|--------|-------------|
| üî¥ N√©gatif | {sentiment_counts['negative']} | {sentiment_counts['negative']/total_articles*100:.1f}% |
| üü° L√©g√®rement n√©gatif | {sentiment_counts['neutral_negative']} | {sentiment_counts['neutral_negative']/total_articles*100:.1f}% |
| ‚ö™ Neutre | {sentiment_counts['neutral']} | {sentiment_counts['neutral']/total_articles*100:.1f}% |
| üü¢ L√©g√®rement positif | {sentiment_counts['neutral_positive']} | {sentiment_counts['neutral_positive']/total_articles*100:.1f}% |
| üü¢ Positif | {sentiment_counts['positive']} | {sentiment_counts['positive']/total_articles*100:.1f}% |

### üì∞ SOURCES PRINCIPALES

{chr(10).join([f'{i+1}. **{source}** - {count} article(s)' for i, (source, count) in enumerate(top_sources)])}

### üéØ ARTICLES R√âCENTS

{chr(10).join([f'{i+1}. **{article["title"]}** ({article.get("source", "Source inconnue")}) - {article.get("pub_date", "Date inconnue")}' for i, article in enumerate(recent_articles)])}

### üîß DIAGNOSTIC

Pour r√©tablir l'analyse IA :
1. V√©rifiez le serveur Mistral : `./server -m models/mistral-7b-v0.2-q4_0.gguf`
2. Testez la connexion : `http://localhost:8080/health`
3. Relancez l'analyse une fois le serveur r√©tabli

---
*Rapport g√©n√©r√© automatiquement par GEOPOL Analytics - {datetime.now().strftime('%d/%m/%Y √† %H:%M')} - Mode d√©grad√©*
"""
        
        return analysis


    # Nouvelle methode pour le chat (miaou-miaou a l'ecran) simple

# Dans LlamaClient - Mettre √† jour generate_chat_response
    def generate_chat_response(self, user_message: str, context: Dict = None) -> Dict:
        """G√©n√®re une r√©ponse de chat simple pour l'assistant - VERSION CORRIG√âE"""
        try:
            # Test de connexion d'abord
            connected, message = self.test_connection()
            if not connected:
                logger.warning(f"‚ùå Serveur Mistral inaccessible: {message}")
                return {
                    'success': False,
                    'error': f'Serveur Mistral inaccessible: {message}',
                    'response': self._get_fallback_chat_response(user_message),
                    'connection_status': message
                }
            
            # Prompt pour l'assistant
            system_prompt = """Tu es GEOPOL Assistant, un expert en g√©opolitique et analyse √©conomique. 
Sois concis, utile et factuel dans tes r√©ponses. R√©ponds en fran√ßais en maximum 10 lignes."""

            user_prompt = f"Utilisateur: {user_message}"
            
            # Construction du prompt au format Mistral
            full_prompt = f"[INST] {system_prompt}\n\n{user_prompt} [/INST]"
            
            request_data = {
                "prompt": full_prompt,
                "temperature": 0.4,
                "top_p": 0.8,
                "max_tokens": 400,
                "repeat_penalty": 1.1,
                "stop": ["</s>", "[INST]", "[/INST]"],
                "stream": False,
                "threads": 8
            }
            
            logger.info(f"üí¨ Chat Mistral - Message: {user_message[:100]}...")
            
            try:
                response = requests.post(
                    f"{self.endpoint}/completion",
                    json=request_data,
                    headers={"Content-Type": "application/json"},
                    timeout=120  # Augment√© √† 120s pour CPU sans GPU
                )
                
                if response.status_code == 200:
                    data = response.json()
                    raw_response = data.get('content', '').strip()
                    
                    # Nettoyer la r√©ponse
                    cleaned_response = self._clean_mistral_response(raw_response)
                    
                    if cleaned_response and len(cleaned_response) > 10:
                        logger.info(f"‚úÖ R√©ponse chat g√©n√©r√©e ({len(cleaned_response)} caract√®res)")
                        return {
                            'success': True,
                            'response': cleaned_response,
                            'model_used': data.get('model', 'mistral-7b-v0.2-q4_0'),
                            'timestamp': datetime.now().isoformat()
                        }
                    else:
                        logger.warning("‚ö†Ô∏è R√©ponse chat vide ou trop courte")
                        raise Exception("R√©ponse vide")
                        
                else:
                    logger.error(f"‚ùå Erreur HTTP {response.status_code}")
                    raise Exception(f"Erreur HTTP {response.status_code}")
                    
            except requests.Timeout:
                logger.error("‚è±Ô∏è Timeout lors du chat")
                raise Exception("Le serveur Mistral met trop de temps √† r√©pondre")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration chat: {e}")
            return {
                'success': False,
                'error': str(e),
                'response': self._get_fallback_chat_response(user_message),
                'connection_status': 'Erreur pendant la g√©n√©ration'
            }
    
    def _get_fallback_chat_response(self, user_message: str) -> str:
        """R√©ponses de fallback pour le chat"""
        fallback_responses = [
            "Je suis d√©sol√©, mon service d'analyse est temporairement indisponible. Vous pouvez consulter les tableaux de bord √©conomiques qui sont pleinement fonctionnels.",
            "Mon syst√®me de r√©ponse intelligente est en maintenance. En attendant, vous pouvez consulter les rapports d'analyse g√©n√©r√©s pr√©c√©demment.",
            "Je rencontre des difficult√©s techniques pour acc√©der √† mon moteur d'analyse. Les donn√©es g√©opolitiques sont toutefois disponibles dans les sections d√©di√©es.",
            "Le serveur Mistral 7B n'est pas accessible actuellement. Vous pouvez essayer de rafra√Æchir la page ou r√©essayer dans quelques minutes.",
            "Pour le moment, je ne peux pas acc√©der √† mon intelligence artificielle. Mais vous pouvez utiliser les outils d'analyse disponibles dans GEOPOL Analytics."
        ]
        
        # Choisir une r√©ponse bas√©e sur le hash du message pour varier
        hash_val = int(hashlib.md5(user_message.encode()).hexdigest(), 16)
        return fallback_responses[hash_val % len(fallback_responses)]
    
    def _clean_mistral_response(self, text: str) -> str:
        """Nettoie la r√©ponse Mistral - Version simplifi√©e pour le chat"""
        if not text:
            return ""
        
        # Supprimer les balises Mistral
        text = text.replace('</s>', '').replace('<s>', '')
        text = text.replace('[INST]', '').replace('[/INST]', '')
        
        # Nettoyer les r√©p√©titions
        lines = text.split('\n')
        clean_lines = []
        
        for line in lines:
            clean_line = line.strip()
            if clean_line and not any(marker in clean_line for marker in ["SYSTEM:", "USER:", "Tu es GEOPOL"]):
                clean_lines.append(clean_line)
        
        return '\n'.join(clean_lines[:10]).strip()  # Limiter √† 10 lignes pour le chat


# Instance globale
_llama_client = None

def get_llama_client(endpoint: str = None) -> LlamaClient:
    """Retourne l'instance singleton du client Llama"""
    global _llama_client
    if _llama_client is None:
        endpoint = endpoint or "http://localhost:8080"
        _llama_client = LlamaClient(endpoint=endpoint)
        
        connected, message = _llama_client.test_connection()
        if connected:
            logger.info("üöÄ Client Mistral 7B initialis√© avec succ√®s")
        else:
            logger.warning(f"‚ö†Ô∏è Client Mistral initialis√© mais serveur inaccessible: {message}")
    
    return _llama_client