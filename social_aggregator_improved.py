# Flask/social_aggregator_improved.py
"""
Agr√©gateur social am√©lior√© avec ciblage g√©ographique et linguistique
Version optimis√©e pour analyse par pays
"""

import requests
import logging
import re
import json
import random
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from collections import Counter
from .database import DatabaseManager
from .sentiment_analyzer import SentimentAnalyzer

logger = logging.getLogger(__name__)

# Configuration des pays surveill√©s
MONITORED_COUNTRIES = {
    'france': {
        'name': 'France',
        'language': 'fr',
        'keywords': ['france', 'fran√ßais', 'paris', 'macron'],
        'hashtags': ['#France', '#Paris', '#Politique'],
        'exclusions': []
    },
    'usa': {
        'name': '√âtats-Unis',
        'language': 'en',
        'keywords': ['usa', 'america', 'us politics', 'washington'],
        'hashtags': ['#USA', '#Politics', '#America'],
        'exclusions': []
    },
    'uk': {
        'name': 'Royaume-Uni',
        'language': 'en',
        'keywords': ['uk', 'britain', 'british', 'london'],
        'hashtags': ['#UK', '#Britain', '#London'],
        'exclusions': []
    },
    'germany': {
        'name': 'Allemagne',
        'language': 'de',
        'keywords': ['deutschland', 'germany', 'berlin'],
        'hashtags': ['#Deutschland', '#Germany'],
        'exclusions': []
    },
    'china': {
        'name': 'Chine',
        'language': 'zh',
        'keywords': ['china', '‰∏≠ÂõΩ', 'beijing'],
        'hashtags': ['#China', '#‰∏≠ÂõΩ'],
        'exclusions': []
    },
    'russia': {
        'name': 'Russie',
        'language': 'ru',
        'keywords': ['russia', '—Ä–æ—Å—Å–∏—è', 'moscow'],
        'hashtags': ['#Russia', '#–†–æ—Å—Å–∏—è'],
        'exclusions': []
    }
}

# Th√®mes √©motionnels prioritaires
EMOTION_THEMES = {
    'anger': {
        'fr': ['col√®re', 'rage', 'furieux', 'indign√©', 'r√©volte'],
        'en': ['anger', 'rage', 'furious', 'outrage', 'mad'],
        'de': ['wut', 'zorn', '√§rger', 'emp√∂rung'],
        'ru': ['–≥–Ω–µ–≤', '—è—Ä–æ—Å—Ç—å', '–∑–ª–æ—Å—Ç—å'],
        'zh': ['ÊÑ§ÊÄí', 'ÁîüÊ∞î', 'Êö¥ÊÄí']
    },
    'fear': {
        'fr': ['peur', 'crainte', 'anxi√©t√©', 'inqui√©tude', 'angoisse'],
        'en': ['fear', 'anxiety', 'worry', 'concern', 'scared'],
        'de': ['angst', 'furcht', 'sorge', 'bedenken'],
        'ru': ['—Å—Ç—Ä–∞—Ö', '–±–æ—è–∑–Ω—å', '—Ç—Ä–µ–≤–æ–≥–∞'],
        'zh': ['ÊÅêÊÉß', 'ÂÆ≥ÊÄï', 'ÊãÖÂøÉ']
    },
    'joy': {
        'fr': ['joie', 'bonheur', 'heureux', 'c√©l√©bration'],
        'en': ['joy', 'happiness', 'happy', 'celebration', 'delight'],
        'de': ['freude', 'gl√ºck', 'fr√∂hlich'],
        'ru': ['—Ä–∞–¥–æ—Å—Ç—å', '—Å—á–∞—Å—Ç—å–µ', '–≤–µ—Å–µ–ª—å–µ'],
        'zh': ['Âø´‰πê', 'È´òÂÖ¥', 'Âπ∏Á¶è']
    },
    'sadness': {
        'fr': ['tristesse', 'peine', 'chagrin', 'd√©pression'],
        'en': ['sadness', 'sad', 'sorrow', 'grief', 'depression'],
        'de': ['traurigkeit', 'trauer', 'kummer'],
        'ru': ['–≥—Ä—É—Å—Ç—å', '–ø–µ—á–∞–ª—å', '—Ç–æ—Å–∫–∞'],
        'zh': ['ÊÇ≤‰º§', 'ÈöæËøá', 'Âøß‰º§']
    }
}

# Instances Nitter (r√©duites aux plus fiables)
NITTER_INSTANCES = [
    'https://nitter.net',
    'https://nitter.privacydev.net',
    'https://nitter.poast.org',
    'https://nitter.fdn.fr'
]

class ImprovedSocialAggregator:
    """
    Agr√©gateur social am√©lior√© avec ciblage g√©ographique
    """
    
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.sentiment_analyzer = SentimentAnalyzer()
        
        # Gestion des instances
        self.nitter_instances = NITTER_INSTANCES.copy()
        self.blacklisted_instances = set()
        self.instance_stats = {inst: {'success': 0, 'errors': 0} for inst in self.nitter_instances}
        
        # Cache pour √©viter les doublons
        self.processed_ids = set()
        
        logger.info(f"üåç ImprovedSocialAggregator initialis√© avec {len(MONITORED_COUNTRIES)} pays surveill√©s")
    
    def fetch_posts_by_country(self, country_code: str, days: int = 1, limit: int = 50) -> Dict[str, Any]:
        """
        R√©cup√®re les posts pour un pays sp√©cifique
        
        Args:
            country_code: Code du pays (ex: 'france', 'usa')
            days: Nombre de jours √† analyser
            limit: Limite de posts par pays
        
        Returns:
            Dictionnaire avec posts et statistiques
        """
        if country_code not in MONITORED_COUNTRIES:
            logger.error(f"‚ùå Pays non surveill√©: {country_code}")
            return {'success': False, 'error': 'Pays non surveill√©'}
        
        country_config = MONITORED_COUNTRIES[country_code]
        logger.info(f"üåç R√©cup√©ration posts pour {country_config['name']}")
        
        # Construire la requ√™te optimis√©e
        posts = self._fetch_targeted_posts(country_config, days, limit)
        
        if not posts:
            return {
                'success': True,
                'country': country_config['name'],
                'posts': [],
                'count': 0
            }
        
        # Analyser les √©motions et sentiments
        analyzed_posts = self._analyze_posts_emotions(posts, country_config)
        
        # Calculer les statistiques
        stats = self._calculate_country_stats(analyzed_posts, country_config)
        
        # Sauvegarder en base
        saved_count = self._save_country_posts(analyzed_posts, country_code)
        
        return {
            'success': True,
            'country': country_config['name'],
            'country_code': country_code,
            'posts': analyzed_posts[:20],  # Retourner seulement les 20 premiers
            'count': len(analyzed_posts),
            'saved_count': saved_count,
            'statistics': stats
        }
    
    def _fetch_targeted_posts(self, country_config: Dict, days: int, limit: int) -> List[Dict[str, Any]]:
        """
        R√©cup√©ration cibl√©e avec requ√™tes optimis√©es par pays
        """
        posts = []
        instance = self._get_best_instance()
        
        if not instance:
            logger.error("‚ùå Aucune instance Nitter disponible")
            return []
        
        # Construire la requ√™te Nitter optimis√©e
        query_parts = []
        
        # Ajouter les mots-cl√©s principaux
        keywords = country_config['keywords'][:3]  # Limiter √† 3 mots-cl√©s max
        query_parts.append(f"({' OR '.join(keywords)})")
        
        # Ajouter la langue
        lang_filter = f"lang:{country_config['language']}"
        
        # Ajouter filtre temporel
        since_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        
        # Construire l'URL
        query = ' '.join(query_parts)
        url = f"{instance}/search"
        
        params = {
            'f': 'tweets',
            'q': f"{query} {lang_filter} since:{since_date} -filter:replies",
            'limit': min(limit, 50)  # Limiter pour √©viter le blocage
        }
        
        logger.info(f"üîç Requ√™te: {query} (langue: {country_config['language']})")
        
        try:
            headers = self._get_headers()
            response = requests.get(url, params=params, headers=headers, timeout=15)
            response.raise_for_status()
            
            # V√©rifier si bloqu√©
            if any(indicator in response.text.lower() for indicator in ['captcha', 'error', 'blocked']):
                logger.warning(f"‚ö†Ô∏è Instance {instance} potentiellement bloqu√©e")
                self.blacklisted_instances.add(instance)
                return []
            
            posts = self._parse_nitter_response(response.text, country_config)
            
            # Mettre √† jour les stats
            self.instance_stats[instance]['success'] += 1
            logger.info(f"‚úÖ {len(posts)} posts r√©cup√©r√©s pour {country_config['name']}")
            
            # Rate limiting
            time.sleep(2)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration posts {country_config['name']}: {e}")
            self.instance_stats[instance]['errors'] += 1
            self.blacklisted_instances.add(instance)
        
        return posts
    
    def _parse_nitter_response(self, html: str, country_config: Dict) -> List[Dict[str, Any]]:
        """
        Parse les r√©sultats Nitter avec filtrage intelligent
        """
        soup = BeautifulSoup(html, 'html.parser')
        posts = []
        
        # S√©lecteurs possibles
        tweet_selectors = ['.main-tweet', '.tweet', '.timeline-item']
        
        tweets = []
        for selector in tweet_selectors:
            tweets = soup.select(selector)
            if tweets:
                break
        
        if not tweets:
            logger.warning("‚ö†Ô∏è Aucun tweet trouv√© dans la r√©ponse")
            return []
        
        for tweet_elem in tweets[:50]:  # Limiter √† 50 max
            try:
                post = self._extract_post_data(tweet_elem, country_config)
                
                if post and self._is_relevant_post(post, country_config):
                    # V√©rifier que ce n'est pas un doublon
                    post_id = post.get('id', '')
                    if post_id not in self.processed_ids:
                        posts.append(post)
                        self.processed_ids.add(post_id)
                        
            except Exception as e:
                logger.debug(f"Erreur extraction post: {e}")
                continue
        
        return posts
    
    def _extract_post_data(self, tweet_elem, country_config: Dict) -> Optional[Dict[str, Any]]:
        """
        Extrait les donn√©es d'un post avec m√©tadonn√©es enrichies
        """
        try:
            # Contenu
            content_selectors = ['.tweet-content', '.tweet-text', '[data-testid="tweetText"]']
            content = ""
            for selector in content_selectors:
                elem = tweet_elem.select_one(selector)
                if elem:
                    content = elem.get_text(strip=True)
                    break
            
            if not content or len(content) < 10:
                return None
            
            # Date
            pub_date = datetime.now()
            date_elem = tweet_elem.select_one('.tweet-date a, time')
            if date_elem:
                date_text = date_elem.get('datetime') or date_elem.get('title')
                if date_text:
                    pub_date = self._parse_date(date_text)
            
            # Auteur
            author = "unknown"
            author_elem = tweet_elem.select_one('.username, .display-name')
            if author_elem:
                author = author_elem.get_text(strip=True)
            
            # URL
            link = ""
            link_elem = tweet_elem.select_one('a[href*="/status/"]')
            if link_elem:
                link = link_elem.get('href', '')
            
            # Engagement (m√©trique importante pour le tri)
            engagement = self._extract_engagement_metrics(tweet_elem)
            
            return {
                'id': f"{country_config['language']}_{hash(content)}_{int(pub_date.timestamp())}",
                'title': content[:100] + '...' if len(content) > 100 else content,
                'content': content,
                'link': link,
                'pub_date': pub_date,
                'author': author,
                'country': country_config['name'],
                'language': country_config['language'],
                'engagement': engagement,
                'relevance_score': 0  # Sera calcul√© apr√®s
            }
            
        except Exception as e:
            logger.debug(f"Erreur extraction: {e}")
            return None
    
    def _extract_engagement_metrics(self, tweet_elem) -> Dict[str, int]:
        """
        Extrait les m√©triques d'engagement (approximatives)
        """
        engagement = {
            'likes': 0,
            'retweets': 0,
            'comments': 0,
            'total_score': 0
        }
        
        # Chercher les statistiques dans le HTML
        stats_elem = tweet_elem.select('.tweet-stats span, .icon-container')
        
        for stat in stats_elem:
            text = stat.get_text(strip=True)
            numbers = re.findall(r'\d+', text)
            if numbers:
                value = int(numbers[0])
                
                # Identifier le type de m√©trique
                if 'comment' in text.lower() or 'üí¨' in text:
                    engagement['comments'] = value
                elif 'retweet' in text.lower() or 'üîÅ' in text:
                    engagement['retweets'] = value
                elif 'like' in text.lower() or '‚ù§Ô∏è' in text:
                    engagement['likes'] = value
        
        # Calculer un score total pond√©r√©
        engagement['total_score'] = (
            engagement['likes'] * 1 +
            engagement['retweets'] * 3 +  # Les retweets comptent plus
            engagement['comments'] * 2
        )
        
        return engagement
    
    def _is_relevant_post(self, post: Dict, country_config: Dict) -> bool:
        """
        Filtre de pertinence du post
        """
        content_lower = post['content'].lower()
        
        # V√©rifier pr√©sence de mots-cl√©s du pays
        has_keyword = any(kw.lower() in content_lower for kw in country_config['keywords'])
        
        # V√©rifier pr√©sence de hashtags pertinents (optionnel)
        has_hashtag = any(ht.lower() in content_lower for ht in country_config['hashtags'])
        
        # V√©rifier exclusions
        has_exclusion = any(ex.lower() in content_lower for ex in country_config.get('exclusions', []))
        
        # Post pertinent si: (keyword OU hashtag) ET PAS d'exclusion
        is_relevant = (has_keyword or has_hashtag) and not has_exclusion
        
        # Bonus: v√©rifier si c'est un post √† fort engagement
        has_engagement = post['engagement']['total_score'] > 10
        
        return is_relevant or has_engagement
    
    def _analyze_posts_emotions(self, posts: List[Dict], country_config: Dict) -> List[Dict]:
        """
        Analyse les √©motions et sentiments des posts
        """
        language = country_config['language']
        analyzed_posts = []
        
        for post in posts:
            try:
                # Analyse de sentiment (RoBERTa)
                sentiment_result = self.sentiment_analyzer.analyze_sentiment(post['content'])
                
                # D√©tection d'√©motions
                detected_emotions = self._detect_emotions(post['content'], language)
                
                # Calcul du score de pertinence final
                relevance_score = self._calculate_relevance_score(
                    post, 
                    detected_emotions, 
                    sentiment_result
                )
                
                analyzed_post = {
                    **post,
                    'sentiment_score': sentiment_result['score'],
                    'sentiment_type': sentiment_result['type'],
                    'sentiment_confidence': sentiment_result['confidence'],
                    'emotions': detected_emotions,
                    'relevance_score': relevance_score
                }
                
                analyzed_posts.append(analyzed_post)
                
            except Exception as e:
                logger.debug(f"Erreur analyse post: {e}")
                analyzed_posts.append({
                    **post,
                    'sentiment_score': 0.0,
                    'sentiment_type': 'neutral',
                    'emotions': {},
                    'relevance_score': 0
                })
        
        # Trier par score de pertinence d√©croissant
        analyzed_posts.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        return analyzed_posts
    
    def _detect_emotions(self, text: str, language: str) -> Dict[str, int]:
        """
        D√©tecte les √©motions pr√©sentes dans le texte
        """
        text_lower = text.lower()
        emotions_detected = {}
        
        for emotion, translations in EMOTION_THEMES.items():
            if language not in translations:
                continue
            
            keywords = translations[language]
            count = sum(text_lower.count(kw.lower()) for kw in keywords)
            
            if count > 0:
                emotions_detected[emotion] = count
        
        return emotions_detected
    
    def _calculate_relevance_score(self, post: Dict, emotions: Dict, sentiment: Dict) -> float:
        """
        Calcule un score de pertinence composite
        """
        score = 0.0
        
        # Score d'engagement (normalis√©)
        engagement_score = min(post['engagement']['total_score'] / 100, 10)
        score += engagement_score * 3  # Poids x3
        
        # Score √©motionnel
        emotion_score = sum(emotions.values()) * 2
        score += emotion_score
        
        # Score de sentiment (intensit√©)
        sentiment_intensity = abs(sentiment['score'])
        score += sentiment_intensity * 5
        
        # Bonus pour sentiment fort
        if sentiment_intensity > 0.5:
            score += 5
        
        # Bonus pour √©motions multiples
        if len(emotions) > 1:
            score += 3
        
        return round(score, 2)
    
    def _calculate_country_stats(self, posts: List[Dict], country_config: Dict) -> Dict[str, Any]:
        """
        Calcule les statistiques pour un pays
        """
        if not posts:
            return {}
        
        # Distribution des sentiments
        sentiments = [p['sentiment_type'] for p in posts]
        sentiment_counts = Counter(sentiments)
        
        # √âmotions dominantes
        all_emotions = []
        for post in posts:
            all_emotions.extend(post.get('emotions', {}).keys())
        emotion_counts = Counter(all_emotions)
        
        # Score moyen de sentiment
        avg_sentiment = sum(p['sentiment_score'] for p in posts) / len(posts)
        
        # Top posts (par engagement)
        top_posts = sorted(posts, key=lambda x: x['engagement']['total_score'], reverse=True)[:5]
        
        return {
            'total_posts': len(posts),
            'sentiment_distribution': {
                'positive': sentiment_counts.get('positive', 0),
                'negative': sentiment_counts.get('negative', 0),
                'neutral_positive': sentiment_counts.get('neutral_positive', 0),
                'neutral_negative': sentiment_counts.get('neutral_negative', 0)
            },
            'average_sentiment': round(avg_sentiment, 3),
            'dominant_emotions': dict(emotion_counts.most_common(5)),
            'top_posts': [
                {
                    'content': p['title'],
                    'engagement': p['engagement']['total_score'],
                    'sentiment': p['sentiment_type']
                }
                for p in top_posts
            ],
            'engagement_stats': {
                'total_likes': sum(p['engagement']['likes'] for p in posts),
                'total_retweets': sum(p['engagement']['retweets'] for p in posts),
                'total_comments': sum(p['engagement']['comments'] for p in posts)
            }
        }
    
    def _save_country_posts(self, posts: List[Dict], country_code: str) -> int:
        """
        Sauvegarde les posts avec m√©tadonn√©es pays
        """
        conn = self.db_manager.get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS social_posts_by_country (
                    id TEXT PRIMARY KEY,
                    country_code TEXT,
                    country_name TEXT,
                    language TEXT,
                    title TEXT,
                    content TEXT,
                    link TEXT,
                    pub_date DATETIME,
                    author TEXT,
                    sentiment_score REAL,
                    sentiment_type TEXT,
                    sentiment_confidence REAL,
                    emotions TEXT,
                    engagement TEXT,
                    relevance_score REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            saved_count = 0
            
            for post in posts:
                try:
                    cursor.execute("""
                        INSERT OR REPLACE INTO social_posts_by_country
                        (id, country_code, country_name, language, title, content, link,
                         pub_date, author, sentiment_score, sentiment_type, sentiment_confidence,
                         emotions, engagement, relevance_score)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        post['id'],
                        country_code,
                        post['country'],
                        post['language'],
                        post['title'],
                        post['content'],
                        post['link'],
                        post['pub_date'],
                        post['author'],
                        post['sentiment_score'],
                        post['sentiment_type'],
                        post['sentiment_confidence'],
                        json.dumps(post.get('emotions', {})),
                        json.dumps(post['engagement']),
                        post['relevance_score']
                    ))
                    saved_count += 1
                except Exception as e:
                    logger.debug(f"Erreur sauvegarde post: {e}")
            
            conn.commit()
            logger.info(f"üíæ {saved_count} posts sauvegard√©s pour {country_code}")
            return saved_count
            
        except Exception as e:
            logger.error(f"Erreur sauvegarde: {e}")
            conn.rollback()
            return 0
        finally:
            conn.close()
    
    def fetch_all_countries(self, days: int = 1, limit_per_country: int = 30) -> Dict[str, Any]:
        """
        R√©cup√®re les posts pour tous les pays surveill√©s
        """
        results = {}
        total_posts = 0
        
        for country_code in MONITORED_COUNTRIES.keys():
            logger.info(f"üåç Traitement de {country_code}...")
            
            result = self.fetch_posts_by_country(country_code, days, limit_per_country)
            
            if result['success']:
                results[country_code] = result
                total_posts += result['count']
            
            # Pause entre pays pour √©viter le rate limiting
            time.sleep(3)
        
        logger.info(f"‚úÖ R√©cup√©ration termin√©e: {total_posts} posts pour {len(results)} pays")
        
        return {
            'success': True,
            'countries': results,
            'total_posts': total_posts,
            'countries_analyzed': len(results)
        }
    
    def get_country_comparison(self, days: int = 7) -> Dict[str, Any]:
        """
        Compare les tendances √©motionnelles entre pays
        """
        conn = self.db_manager.get_connection()
        cursor = conn.cursor()
        
        cutoff_date = datetime.now() - timedelta(days=days)
        
        cursor.execute("""
            SELECT 
                country_code,
                country_name,
                AVG(sentiment_score) as avg_sentiment,
                COUNT(*) as post_count,
                AVG(relevance_score) as avg_relevance
            FROM social_posts_by_country
            WHERE pub_date >= ?
            GROUP BY country_code, country_name
            ORDER BY avg_sentiment DESC
        """, (cutoff_date,))
        
        countries_data = []
        for row in cursor.fetchall():
            countries_data.append({
                'country_code': row[0],
                'country': row[1],
                'avg_sentiment': round(row[2], 3),
                'post_count': row[3],
                'avg_relevance': round(row[4], 2)
            })
        
        conn.close()
        
        return {
            'success': True,
            'countries': countries_data,
            'period_days': days
        }
    
    # M√©thodes utilitaires
    def _get_best_instance(self) -> Optional[str]:
        """Retourne la meilleure instance disponible"""
        available = [inst for inst in self.nitter_instances if inst not in self.blacklisted_instances]
        
        if not available:
            self.blacklisted_instances.clear()
            available = self.nitter_instances.copy()
            logger.warning("üîÑ Reset blacklist instances")
        
        return random.choice(available) if available else None
    
    def _get_headers(self) -> Dict[str, str]:
        """Headers HTTP r√©alistes"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        ]
        
        return {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive'
        }
    
    def _parse_date(self, date_text: str) -> datetime:
        """Parse diff√©rents formats de dates"""
        try:
            if 'T' in date_text or 'Z' in date_text:
                return datetime.fromisoformat(date_text.replace('Z', '+00:00'))
            return datetime.now()
        except:
            return datetime.now()

# Instance globale
_improved_aggregator = None

def get_improved_aggregator(db_manager: DatabaseManager) -> ImprovedSocialAggregator:
    """Retourne l'instance singleton"""
    global _improved_aggregator
    if _improved_aggregator is None:
        _improved_aggregator = ImprovedSocialAggregator(db_manager)
    return _improved_aggregator